{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Busola181/Deep-Learning-CNN-Projects/blob/main/Image_captioning_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfIkCvNe6hST"
      },
      "source": [
        "MOUNTING DRIVE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InVnHHbu6Xqa",
        "outputId": "3ab1a206-5d96-4b2f-aef0-359a67676bdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjRcP_IW6nlW"
      },
      "source": [
        "EXTRACTING DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfZBuBQU62K2"
      },
      "outputs": [],
      "source": [
        "API_KEY_PATH=\"/content/drive/MyDrive/kaggle.json\"\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp $API_KEY_PATH ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d kunalgupta2616/flickr-8k-images-with-captions\n",
        "\n",
        "RESOURCES_PATH=\"/content/flickr-image-dataset.zip\"\n",
        "!cp $RESOURCES_PATH.\n",
        "!unzip /content/flickr-8k-images-with-captions.zip -d /content/new_folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SEKcGjB8Dyd"
      },
      "source": [
        "IMPORT LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qc88cEaUg3Jt"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade sympy\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import random_split, DataLoader, Dataset\n",
        "import zipfile\n",
        "import re\n",
        "import nltk\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "from PIL import Image\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "from torchsummary import summary\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torchvision.io import read_image\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "\n",
        "nltk.data.path.append('/usr/local/share/nltk_data')\n",
        "nltk.download('wordnet', download_dir='/usr/local/share/nltk_data')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "captions_path = \"/content/new_folder/captions.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdJv61RghRt-",
        "outputId": "b5b85806-7ea0-4e97-9d67-dc2fe06dec41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 9094\n",
            "Sample Vocabulary: ['<PAD>', '<BOS>', '<EOS>', '<UNK>', 'a', 'aa', 'abandon', 'abandoned', 'abarrotes', 'abdomen', 'ability', 'aboard', 'aboriginal', 'about', 'above', 'abseiling', 'abspedestrians', 'ac', 'accelerates', 'accends']\n"
          ]
        }
      ],
      "source": [
        "data = {'image_name': [], 'caption': []}\n",
        "with open(captions_path, 'r') as f:\n",
        "    for idx, line in enumerate(f):\n",
        "        if idx == 0:\n",
        "            continue\n",
        "        parts = [line.strip()[:line.find(',')], line.strip()[line.find(',')+1:].replace('\"', '')]\n",
        "        if len(parts) == 2:\n",
        "            image_name, caption = parts\n",
        "            data['image_name'].append(image_name)\n",
        "            data['caption'].append(caption)\n",
        "# print(data)\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# texts = \" \".join(df['caption'].astype(str).tolist())\n",
        "\n",
        "texts = \"\"\n",
        "for caption in data[\"caption\"]:\n",
        "    texts += caption\n",
        "cleaned_texts = re.sub(r'[^a-zA-Z\\s]', ' ', texts).lower()\n",
        "words = word_tokenize(cleaned_texts)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "vocab = sorted(set(lemmatized_words))\n",
        "vocab.insert(0, '<PAD>')\n",
        "vocab.insert(1, '<BOS>')\n",
        "vocab.insert(2, '<EOS>')\n",
        "vocab.insert(3, '<UNK>')\n",
        "\n",
        "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "idx_to_word = {idx: word for idx, word in enumerate(vocab)}\n",
        "\n",
        "print(\"Vocabulary Size:\", len(vocab))\n",
        "print(\"Sample Vocabulary:\", vocab[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3n3-7HFd5xB5"
      },
      "source": [
        "HYPER PARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxCSVueMklNE",
        "outputId": "a676c544-78cb-4daa-a3b2-9ad8bdaa5b09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "embed_size = 1024\n",
        "hidden_size = 1024\n",
        "vocab_size = len(vocab)\n",
        "num_layers = 3\n",
        "learning_rate = 1e-4\n",
        "epochs = 100\n",
        "num_workers = 2\n",
        "batch_size = 64\n",
        "max_caption_length = 50\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aWFYI435r5z"
      },
      "source": [
        "IMAGE DATA PREPARATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEUxFoeUs3wm",
        "outputId": "8b8e5d70-9f89-4ae6-9dcf-70aa0f21066f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[[0.3216, 0.4353, 0.4549,  ..., 0.0157, 0.0235, 0.0235],\n",
            "         [0.3098, 0.4431, 0.4667,  ..., 0.0314, 0.0275, 0.0471],\n",
            "         [0.3020, 0.4588, 0.4745,  ..., 0.0314, 0.0275, 0.0392],\n",
            "         ...,\n",
            "         [0.7294, 0.5882, 0.6706,  ..., 0.8314, 0.6471, 0.6471],\n",
            "         [0.6902, 0.6941, 0.8627,  ..., 0.8235, 0.6588, 0.6588],\n",
            "         [0.8118, 0.8196, 0.7333,  ..., 0.8039, 0.6549, 0.6627]],\n",
            "\n",
            "        [[0.3412, 0.5020, 0.5255,  ..., 0.0118, 0.0235, 0.0314],\n",
            "         [0.3294, 0.5059, 0.5412,  ..., 0.0353, 0.0392, 0.0824],\n",
            "         [0.3098, 0.5176, 0.5529,  ..., 0.0353, 0.0510, 0.0863],\n",
            "         ...,\n",
            "         [0.4235, 0.3137, 0.4784,  ..., 0.8667, 0.7255, 0.7216],\n",
            "         [0.3765, 0.5059, 0.6627,  ..., 0.8549, 0.7216, 0.7216],\n",
            "         [0.4941, 0.5804, 0.4784,  ..., 0.8392, 0.7216, 0.7216]],\n",
            "\n",
            "        [[0.3804, 0.4902, 0.4980,  ..., 0.0118, 0.0157, 0.0196],\n",
            "         [0.3608, 0.5059, 0.5176,  ..., 0.0275, 0.0235, 0.0235],\n",
            "         [0.3647, 0.5255, 0.5333,  ..., 0.0196, 0.0235, 0.0275],\n",
            "         ...,\n",
            "         [0.1216, 0.1098, 0.2549,  ..., 0.9176, 0.8235, 0.7961],\n",
            "         [0.0784, 0.1804, 0.2902,  ..., 0.9137, 0.8118, 0.7843],\n",
            "         [0.1843, 0.2588, 0.2824,  ..., 0.9176, 0.8039, 0.7686]]]), tensor([   4, 1465, 3943,    4, 5713, 2380, 4037, 1580, 8496,    4, 6792, 5178,\n",
            "        7488, 3943,  190, 2603, 8787,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0]))\n",
            "40455\n",
            "DataLoader setup complete.\n"
          ]
        }
      ],
      "source": [
        "class ICDataset(Dataset):\n",
        "    img_base_folder = \"/content/new_folder/Images\"\n",
        "    def __init__(self, captions_file, transforms=None):\n",
        "        super(ICDataset, self).__init__()\n",
        "        self.dataset_metadata = self.load_captions(captions_file)\n",
        "        self.transforms = transforms\n",
        "        self.max_caption_length = 50\n",
        "\n",
        "    def load_captions(self, captions_file):\n",
        "         metadata = []\n",
        "         with open(captions_file, 'r') as f:\n",
        "            for idx, line in enumerate(f):\n",
        "                if idx == 0:\n",
        "                    continue\n",
        "                parts = [line.strip()[:line.find(',')], line.strip()[line.find(',')+1:].replace('\"', '')]\n",
        "                if len(parts) == 2:\n",
        "                    filename, caption = parts\n",
        "                    filename = filename.split(\"#\")[0]\n",
        "                    metadata.append([filename, caption])\n",
        "                else:\n",
        "                    print(f\"Skipping malformed line: {line.strip()}\")\n",
        "         return metadata\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_name, caption = self.dataset_metadata[idx]\n",
        "        image_path = os.path.join(self.img_base_folder, file_name)\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        if self.transforms:\n",
        "            image = self.transforms(image)\n",
        "        target = self.parse_caption(caption)\n",
        "        return image, target\n",
        "\n",
        "    def parse_caption(self, caption=\"\"):\n",
        "        cleaned_text = re.sub(r'[^a-zA-Z\\s]', ' ', caption).lower()\n",
        "        words = word_tokenize(cleaned_text)\n",
        "        lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "        input_indices = [word_to_idx.get(word, word_to_idx['<UNK>']) for word in lemmatized_words]\n",
        "        input_vector = torch.tensor(input_indices)\n",
        "        if len(input_vector) > self.max_caption_length:\n",
        "            input_vector = input_vector[:self.max_caption_length]\n",
        "        else:\n",
        "            padding = [word_to_idx['<PAD>']] * (self.max_caption_length - len(input_vector))\n",
        "            input_vector = torch.cat((input_vector, torch.tensor(padding)))\n",
        "        return input_vector\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset_metadata)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "dataset = ICDataset(captions_path, transforms=transform)\n",
        "print(dataset[0])\n",
        "print(len(dataset))\n",
        "train_ds, valid_ds, test_ds = random_split(dataset, [28318, 6069, 6068])\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=False)\n",
        "test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(\"DataLoader setup complete.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMzPmMKb6NvI"
      },
      "outputs": [],
      "source": [
        "img_base_folder = \"/content/new_folder/Images\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBzjotwPB0xm"
      },
      "source": [
        "TRANSFER LEARNING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lchy2pOI2J8S"
      },
      "outputs": [],
      "source": [
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        resnet = models.resnet50(pretrained=True)\n",
        "        for param in resnet.parameters():\n",
        "            param.requires_grad_(False)\n",
        "\n",
        "        modules = list(resnet.children())[:-1]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
        "\n",
        "    def forward(self, images):\n",
        "        features = self.resnet(images)\n",
        "        features = features.view(features.size(0), -1)\n",
        "        features = self.embed(features)\n",
        "        return features\n",
        "\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_size\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        self.hidden = (torch.zeros(1, 1, hidden_size), torch.zeros(1, 1, hidden_size))\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        cap_embedding = self.embed(\n",
        "            captions[:, :-1]\n",
        "        )\n",
        "        embeddings = torch.cat((features.unsqueeze(dim=1), cap_embedding), dim=1)\n",
        "        lstm_out, self.hidden = self.lstm(\n",
        "            embeddings\n",
        "        )\n",
        "        outputs = self.linear(lstm_out)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def sample(self, inputs, states=None, max_len=20):\n",
        "        res = []\n",
        "\n",
        "        for i in range(max_len):\n",
        "            lstm_out, states = self.lstm(\n",
        "                inputs, states\n",
        "            )\n",
        "            outputs = self.linear(lstm_out.squeeze(dim=1))\n",
        "            _, predicted_idx = outputs.max(dim=1)\n",
        "            res.append(predicted_idx.item())\n",
        "            if predicted_idx == 1:\n",
        "                break\n",
        "            inputs = self.embed(predicted_idx)\n",
        "            inputs = inputs.unsqueeze(1)\n",
        "\n",
        "        return res"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATASETS LENGTH"
      ],
      "metadata": {
        "id": "gWF6nadNVkqj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B08ixo_5FYRJ",
        "outputId": "e292d6d3-cd82-4353-8a79-a4a371cef6cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28318\n",
            "6069\n",
            "6068\n"
          ]
        }
      ],
      "source": [
        "print(len(train_ds))\n",
        "print(len(valid_ds))\n",
        "print(len(test_ds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHLhWpYYGsyI"
      },
      "source": [
        "TRANSFER LEARNING {LOADING THE PRETRAINED MODEL, FEATURE VECTOR EXTRACTION}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhoFYMSwXNat"
      },
      "outputs": [],
      "source": [
        "class CNNtoRNN_attention(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=3):\n",
        "        super(CNNtoRNN_attention, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.lstm = nn.LSTM(embed_size + hidden_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.attention = nn.Linear(hidden_size + embed_size, 1)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def calculate_attention(self, features, hidden):\n",
        "        if features.dim() == 2:\n",
        "            features = features.unsqueeze(1)\n",
        "\n",
        "        hidden = hidden.unsqueeze(1).expand(-1, features.size(1), -1)\n",
        "\n",
        "        combined = torch.cat((features, hidden), dim=2)\n",
        "        attention_scores = self.attention(combined)\n",
        "        attention_weights = F.softmax(attention_scores, dim=1)\n",
        "        return attention_weights\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        embeddings = self.dropout(self.embed(captions))\n",
        "\n",
        "        hidden_state = torch.zeros((self.num_layers, features.size(0), self.hidden_size)).to(features.device)\n",
        "        cell_state = torch.zeros((self.num_layers, features.size(0), self.hidden_size)).to(features.device)\n",
        "\n",
        "        outputs = []\n",
        "        for t in range(captions.size(1)):\n",
        "            attention_weights = self.calculate_attention(features, hidden_state[-1])\n",
        "            context_vector = torch.sum(attention_weights * features, dim=1)\n",
        "\n",
        "            lstm_input = torch.cat((context_vector.unsqueeze(1), embeddings[:, t].unsqueeze(1)), dim=2)\n",
        "\n",
        "            output, (hidden_state, cell_state) = self.lstm(lstm_input, (hidden_state, cell_state))\n",
        "            output = self.fc(output.squeeze(1))\n",
        "            outputs.append(output)\n",
        "\n",
        "        outputs = torch.stack(outputs, dim=1)\n",
        "        return outputs\n",
        "\n",
        "    def sample(self, features, max_len=20):\n",
        "        \"\"\"Generates a caption for an input feature vector.\"\"\"\n",
        "        sampled_indices = []\n",
        "        hidden_state = torch.zeros((self.num_layers, 1, self.hidden_size)).to(features.device)\n",
        "        cell_state = torch.zeros((self.num_layers, 1, self.hidden_size)).to(features.device)\n",
        "\n",
        "        input_word = torch.tensor([word_to_idx['<BOS>']]).to(features.device)  # Start with the beginning-of-sequence token\n",
        "        input_word = self.embed(input_word).unsqueeze(1)  # Embedding and add batch dimension\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            attention_weights = self.calculate_attention(features, hidden_state[-1])\n",
        "            context_vector = torch.sum(attention_weights * features, dim=1).unsqueeze(1)\n",
        "            lstm_input = torch.cat((context_vector, input_word), dim=2)\n",
        "\n",
        "            output, (hidden_state, cell_state) = self.lstm(lstm_input, (hidden_state, cell_state))\n",
        "            output = self.fc(output.squeeze(1))\n",
        "            predicted_idx = output.argmax(dim=1).item()\n",
        "\n",
        "            sampled_indices.append(predicted_idx)\n",
        "\n",
        "            if predicted_idx == word_to_idx['<EOS>']:\n",
        "                break\n",
        "\n",
        "            input_word = self.embed(torch.tensor([predicted_idx]).to(features.device)).unsqueeze(1)\n",
        "\n",
        "        return sampled_indices\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ATTENTION MECHANISM"
      ],
      "metadata": {
        "id": "PFdcABHYVtt-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eumPY7KRXcqc",
        "outputId": "9cbfc846-a405-4180-9d9a-97dd9f551780"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 137MB/s]\n"
          ]
        }
      ],
      "source": [
        "encoder = EncoderCNN(embed_size).to(device)\n",
        "decoder = CNNtoRNN_attention(embed_size, hidden_size, vocab_size, num_layers).to(device)\n",
        "model = CNNtoRNN_attention(embed_size,hidden_size,vocab_size,num_layers).to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index= word_to_idx['<PAD>'])\n",
        "params = list(encoder.parameters()) + list(decoder.parameters())\n",
        "optimizer = optim.Adam(params, lr= learning_rate)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuvGN0gI5OPi"
      },
      "source": [
        "TRAINING THE DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObJnMMh06ltb"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = '/content/drive/MyDrive/checkpoints'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "# Define the checkpoint\n",
        "checkpoint_path = os.path.join(checkpoint_dir, 'model_checkpoint.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ib_-TN-RSGR"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = '/content/drive/MyDrive/checkpoints/model_checkpoint.pth'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dZVqYICaPd3"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(encoder, decoder, epoch, optimizer, train_loss, val_loss, checkpoint_path):\n",
        "    checkpoint = {\n",
        "                'epoch': epoch + 1,\n",
        "                'encoder_state_dict': encoder.state_dict(),\n",
        "                'decoder_state_dict': decoder.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'train_loss': train_loss,\n",
        "                'val_loss': val_loss\n",
        "            }\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "    print(f'checkpoint saved at {epoch + 1}')\n",
        "\n",
        "def load_checkpoint(encoder, decoder, optimizer, checkpoint_path, device):\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "        encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
        "        decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        epoch = checkpoint['epoch']\n",
        "        train_loss = checkpoint['train_loss']\n",
        "        val_loss = checkpoint['val_loss']\n",
        "        print(f\"Resuming from epoch {epoch + 1} with train loss {train_loss:.4f} and val loss {val_loss:.4f}\")\n",
        "        return epoch + 1\n",
        "    else:\n",
        "        print(\"No checkpoint starting from scratch\")\n",
        "        return 0\n",
        "\n",
        "def train_captioning(encoder, decoder, train_dl, val_dl, epochs, criterion, optimizer, device, vocab_size, checkpoint_path):\n",
        "    encoder.to(device)\n",
        "    decoder.to(device)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "    start_epoch = load_checkpoint(encoder, decoder, optimizer, checkpoint_path, device)\n",
        "\n",
        "\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "        encoder.train()\n",
        "        decoder.train()\n",
        "        epoch_train_loss = 0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for images, captions in train_dl:\n",
        "            images, captions = images.to(device), captions.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            features = encoder(images)\n",
        "            outputs = decoder(features, captions[:, :-1])\n",
        "\n",
        "\n",
        "            loss = criterion(outputs.view(-1, vocab_size), captions[:, 1:].reshape(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_train_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 2)\n",
        "            correct_train += (predicted == captions[:, 1:]).sum().item()\n",
        "            total_train += captions[:, 1:].numel()\n",
        "\n",
        "        avg_train_loss = epoch_train_loss / len(train_dl)\n",
        "        train_accuracy = 100 * correct_train / total_train\n",
        "\n",
        "        encoder.eval()\n",
        "        decoder.eval()\n",
        "        epoch_val_loss = 0\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, captions in val_dl:\n",
        "                images, captions = images.to(device), captions.to(device)\n",
        "                features = encoder(images)\n",
        "                outputs = decoder(features, captions[:, :-1])\n",
        "\n",
        "                loss = criterion(outputs.view(-1, vocab_size), captions[:, 1:].reshape(-1))\n",
        "                epoch_val_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs, 2)\n",
        "                correct_val += (predicted == captions[:, 1:]).sum().item()\n",
        "                total_val += captions[:, 1:].numel()\n",
        "\n",
        "        avg_val_loss = epoch_val_loss / len(val_dl)\n",
        "        val_accuracy = 100 * correct_val / total_val\n",
        "\n",
        "        if (epoch + 1) % 1 == 0:\n",
        "            print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, '\n",
        "                  f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%')\n",
        "            save_checkpoint(encoder, decoder,epoch, optimizer, avg_train_loss, avg_val_loss, checkpoint_path)\n",
        "\n",
        "            scheduler.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8jLhtuBrfNk",
        "outputId": "9acddf36-6e9f-4cff-e904-7aab33298c37"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-5a690bb36b23>:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resuming from epoch 59 with train loss 5.2965 and val loss 5.2983\n",
            "Epoch 60/100, Train Loss: 5.2994, Train Acc: 2.39%, Val Loss: 5.2843, Val Acc: 2.39%\n",
            "checkpoint saved at 60\n",
            "Epoch 61/100, Train Loss: 5.2990, Train Acc: 2.39%, Val Loss: 5.2844, Val Acc: 2.39%\n",
            "checkpoint saved at 61\n",
            "Epoch 62/100, Train Loss: 5.2982, Train Acc: 2.39%, Val Loss: 5.2844, Val Acc: 2.39%\n",
            "checkpoint saved at 62\n",
            "Epoch 63/100, Train Loss: 5.2983, Train Acc: 2.39%, Val Loss: 5.2844, Val Acc: 2.39%\n",
            "checkpoint saved at 63\n",
            "Epoch 64/100, Train Loss: 5.2985, Train Acc: 2.39%, Val Loss: 5.2845, Val Acc: 2.39%\n",
            "checkpoint saved at 64\n",
            "Epoch 65/100, Train Loss: 5.2986, Train Acc: 2.39%, Val Loss: 5.2845, Val Acc: 2.39%\n",
            "checkpoint saved at 65\n",
            "Epoch 66/100, Train Loss: 5.2988, Train Acc: 2.39%, Val Loss: 5.2846, Val Acc: 2.39%\n",
            "checkpoint saved at 66\n",
            "Epoch 67/100, Train Loss: 5.2987, Train Acc: 2.39%, Val Loss: 5.2846, Val Acc: 2.39%\n",
            "checkpoint saved at 67\n",
            "Epoch 68/100, Train Loss: 5.2985, Train Acc: 2.39%, Val Loss: 5.2847, Val Acc: 2.39%\n",
            "checkpoint saved at 68\n",
            "Epoch 69/100, Train Loss: 5.2983, Train Acc: 2.39%, Val Loss: 5.2848, Val Acc: 2.39%\n",
            "checkpoint saved at 69\n",
            "Epoch 70/100, Train Loss: 5.2981, Train Acc: 2.39%, Val Loss: 5.2848, Val Acc: 2.39%\n",
            "checkpoint saved at 70\n",
            "Epoch 71/100, Train Loss: 5.2981, Train Acc: 2.39%, Val Loss: 5.2848, Val Acc: 2.39%\n",
            "checkpoint saved at 71\n",
            "Epoch 72/100, Train Loss: 5.2981, Train Acc: 2.39%, Val Loss: 5.2848, Val Acc: 2.39%\n",
            "checkpoint saved at 72\n",
            "Epoch 73/100, Train Loss: 5.2984, Train Acc: 2.39%, Val Loss: 5.2848, Val Acc: 2.39%\n",
            "checkpoint saved at 73\n",
            "Epoch 74/100, Train Loss: 5.2983, Train Acc: 2.39%, Val Loss: 5.2848, Val Acc: 2.39%\n",
            "checkpoint saved at 74\n",
            "Epoch 75/100, Train Loss: 5.2986, Train Acc: 2.39%, Val Loss: 5.2848, Val Acc: 2.39%\n",
            "checkpoint saved at 75\n",
            "Epoch 76/100, Train Loss: 5.2985, Train Acc: 2.39%, Val Loss: 5.2848, Val Acc: 2.39%\n",
            "checkpoint saved at 76\n",
            "Epoch 77/100, Train Loss: 5.2984, Train Acc: 2.39%, Val Loss: 5.2848, Val Acc: 2.39%\n",
            "checkpoint saved at 77\n",
            "Epoch 78/100, Train Loss: 5.2983, Train Acc: 2.39%, Val Loss: 5.2848, Val Acc: 2.39%\n",
            "checkpoint saved at 78\n"
          ]
        }
      ],
      "source": [
        "train_captioning(encoder, decoder, train_dl, val_dl, epochs, criterion, optimizer, device, vocab_size, checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suXY1cdGx6zw"
      },
      "outputs": [],
      "source": [
        "image_path = '/content/new_folder/Images/1001773457_577c3a7d70.jpg'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKuR-9qM5aQ8"
      },
      "source": [
        "INFERENCE FUNCTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZ8BlldDxp8i"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()])\n",
        "\n",
        "def evaluate_model(encoder, decoder, test_dl, criterion, vocab_size, max_caption_length, device, idx_to_word, word_to_idx, image_path=image_path, transforms=transform):\n",
        "\n",
        "\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    test_loss = 0\n",
        "    correct_test = 0\n",
        "    total_test = 0\n",
        "    generated_captions = []\n",
        "    ground_truth_captions = []\n",
        "\n",
        "    # If an image_path is provided, generate a caption for it\n",
        "    if image_path:\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        if transforms:\n",
        "            image = transforms(image)\n",
        "        image = image.unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            features = encoder(image)\n",
        "            sampled_caption = decoder.sample(features, max_len=max_caption_length)\n",
        "\n",
        "        caption_text = ' '.join([idx_to_word[idx] for idx in sampled_caption if idx not in [word_to_idx['<PAD>'], word_to_idx['<BOS>'], word_to_idx['<EOS>']]])\n",
        "\n",
        "        plt.imshow(Image.open(image_path))\n",
        "        #plt.figure(figsize=(8, 8))\n",
        "        plt.title(\"Generated Caption: \" + caption_text)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "        return\n",
        "\n",
        "    # Otherwise, evaluate on the entire test dataset\n",
        "    with torch.no_grad():\n",
        "        for images, captions in test_dl:\n",
        "            images, captions = images.to(device), captions.to(device)\n",
        "            features = encoder(images)\n",
        "            outputs = decoder(features, captions[:, :-1])\n",
        "\n",
        "            # Calculate loss and accuracy\n",
        "            loss = criterion(outputs.view(-1, vocab_size), captions[:, 1:].reshape(-1))\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 2)\n",
        "            correct_test += (predicted == captions[:, 1:]).sum().item()\n",
        "            total_test += captions[:, 1:].numel()\n",
        "\n",
        "            # Store generated and ground truth captions for BLEU score calculation\n",
        "            for i in range(images.size(0)):\n",
        "                generated_caption = decoder.sample(features[i].unsqueeze(0), max_len=max_caption_length)\n",
        "                generated_text = ' '.join([idx_to_word[idx] for idx in generated_caption])\n",
        "                ground_truth_text = ' '.join([idx_to_word[idx] for idx in captions[i].cpu().numpy() if idx not in [word_to_idx['<PAD>'], word_to_idx['<BOS>'], word_to_idx['<EOS>']]])\n",
        "\n",
        "                generated_captions.append(generated_text.split())\n",
        "                ground_truth_captions.append([ground_truth_text.split()])\n",
        "\n",
        "    # Average test loss and accuracy\n",
        "    avg_test_loss = test_loss / len(test_dl)\n",
        "    test_accuracy = 100 * correct_test / total_test\n",
        "    print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    bleu_score = sum([sentence_bleu(gt, pred) for gt, pred in zip(ground_truth_captions, generated_captions)]) / len(generated_captions)\n",
        "    print(f\"BLEU Score: {bleu_score:.4f}\")\n",
        "\n",
        "    for i in range(5):\n",
        "        img, caption = next(iter(test_dl))\n",
        "        img, caption = img.to(device), caption.to(device)\n",
        "\n",
        "        features = encoder(img)\n",
        "        sampled_caption = decoder.sample(features[0].unsqueeze(0), max_len=max_caption_length)\n",
        "        caption_text = ' '.join([idx_to_word[idx] for idx in sampled_caption])\n",
        "\n",
        "        plt.imshow(img[0].cpu().permute(1, 2, 0))\n",
        "        plt.title(\"Generated Caption: \" + caption_text)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HWNpV6myQEg"
      },
      "outputs": [],
      "source": [
        "evaluate_model(encoder, decoder, test_dl, criterion, vocab_size, max_caption_length, device, idx_to_word, word_to_idx, image_path=\"/content/new_folder/Images/1001773457_577c3a7d70.jpg\", transforms=transform)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMWYs/PyONrETK//I+aRGIM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}